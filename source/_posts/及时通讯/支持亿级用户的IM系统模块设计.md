---
title: 支持亿级消息的IM系统模块设计
date: 2021-03-23 10:04:23
tags:
	- 及时通讯
	- 系统架构
categories:
    - 及时通讯
abstract: 文章还在路上,敬请期待
---

# 支持亿级用户的IM系统模块设计

现代实现的及时通讯系统一般由长连接和短连接配合使用进行实现，一般的实现架构即Connector和Service两个部分,分别对应长连接服务和短连接服务，**长连接服务主要用于服务端对客户端的推送和新消息到达的通知**，**短连接服务主要用于客户端对于消息的拉取和离线(在线)消息的同步**。当用户数据量增大时,其很容易就会出现以下的缺陷:

- **复杂性高**：每个模块服务包含的功能职责增多、模块的边界模糊、 依赖关系不清晰、 代码质量参差不齐、 混乱地堆砌在一起。
- **可靠性缺失**： 某个核心服务出现问题，例如死循环、内存溢出等， 可能会导致整个系统不能正常使用。
- **拓展成本增大**： 当用户体量增大时,部分服务功能会成为系统瓶颈,当进行服务扩容时,因为部分功能瓶颈的原因，就需要将整个服务进行拓展，带来不需要的成本开销。

为此就需要对常见IM系统架构进行更加细化的服务拆分,以下就IM服务功能拆分提供参考

<!-- more -->

## 技术指标

- **高可靠**：确保不丢消息；
- **高可用**：任意机房或者服务器挂掉，不影响服务；
- **实时性**：不管用户在哪里，在线用户消息在1秒内达到（我们实际是75%消息可以做到120ms）；
- **有序性**：确保用户消息的有序性，不会出现发送和接受的乱序。

## 架构拆分

当需要满足亿级用户量的IM架构时，需要将IM服务的进行更加精细化的服务拆分

- **业务系统**：服务及时通讯的业务逻辑
- **信令系统**：负责用户登录，用户在线状态的维护，以及在线用户的下行推送
- **推送系统**：负责消息的在线推送和离线推送
- **持久化系统**：负责消息和文件的存储和查询

![服务拆分](https://img.hellobyebye.com/doc/2021032318255616164951561616495156116dGPQS7.png)

### **业务系统**

业务系统的详细来说在于**专注处理IM相关的业务逻辑**，主要使用短连接进行实现(HTTP协议)

- 消息发送的鉴权和支持(optional)：客户端通过短连接发送消息
- 维护用户数据：用户基本信息等
- 维护好友关系：好友请求、好友列表、好友信息等
- 维护群组信息：群创建、解散、成员管理等
- 提供数据：在线消息(optional)、离线拉取、历史记录同步
- 其它逻辑：比如通过存储和推送系统，存储消息和发送通知

其功能和实际的架构可以根据具体的业务量和自身的系统架构进行更加细化的服务细分

###  **信令系统**

对于IM系统来说，是最主要和最核心的需求和职责，主要维护服务端与客户端的长连接(socket)，其主要职责包含以下3个部分

#### **维护用户在线状态**

当用户体量大时，需要建立大量的Socket连接和用户接入，为此单台机器是不能满足业务需求的，其落地和必然是集群化的服务，多台服务未用户提供连接服务。当用户A需要给用户B发送消息时,是需要知道用户B当前连接上了那个服务器上，然后通过该台服务器上的Socket连接向用户发送下行通知信息。

#### **下行通知数据下发**

与维护用户在线状态的职责相关，当用户在线的时，如果有其它用户给他发消息，就需要走在线的系统推送，直接将新消息告知到客户端，而不是走离线推送信息。

#### **业务协议内容的分发**

信令服务不只可以处理IM请求，也可以处理其它类型的业务请求。为了处理不同的业务，就需要有分发能力。一般来说会使用Protobuf定义下发的业务协议，其中可能会包括SVID(Service Id)或是CMD(Commond)字段来标识消息数据类型，用户通过该标识能表明当前业务包的功能(eg:心跳和连接授权、QoS、消息到达、用户特权变更等)并交由业务服务进行处理。

这样不管用户有多少的业务或是多少的实现表达，客户端与服务器端仅需要维护一个Socket连接。

#### **信令服务的服务拆分**

信令系统为了实现以上这3个职责，同时要确保我们服务可平行扩展的能力和稳定性，在实际的技术实现上，我们实际上把信令服务分拆为3个服务模块。

**- 连接授权和维护(Login服务)**

- *主要负责维护用户长链接：*

客户端与服务器之间长连接的维护、长连接的连接认证、连接的心跳。用户完成业务模块的登陆操作后，会持有登陆完成后授权给用户Token或者相关凭证、Login服务会接受客户端的连接并对用户心跳包持有的Token进行验证、如果成功就将认证结果发送到Online服务，当授权验证不通过时,直接关闭该次连接。

Login对并发要求比较高，一般要支持TCP+UDP+Websocket几种方式，单服务可以做到10-250万之间。从服务稳定性角度触发，建议是控制VM的CPU/内存，单服务器以20-50万为合适。

Login服务器本身没有状态，任何一个Login服务断掉，客户端检测到以后重连另一个Login服务器并发送心跳包，对整体服务可靠性基本没有影响。

**- 在线信息的维护和同步(Online服务)**

- *主要负责维护用户的在线信息:*

当用户掉线或者根本不在线，Online服务里用户相关信息就是空

当用户在线时，Online就能找到用户登录在哪个集群，哪个Login服务器上

多个Login服务器会连接到Online，定期同步用户登录和离线信息，把用户状态信息存储在共有的存储服务中(Redis集群)。因此也是无状态的，任何一个Online服务挂掉，不影响整体服务能力。当集群规模不大，用户规模也不大时，Online服务也可以收到Login服务里去。

当规模比较大，建议分拆出来，一方面简化Login的逻辑复杂度，同时避免写共有的存储服务的慢操作，在Login服务里。因为Login要同时处理50万以上的并发链接，不适合在循环里嵌入慢操作。

**- 协议的解析和业务的路由(Route服务)**

- *主要负责信令系统跟其它子系统的交互：*

Route下接Login服务，可以接受用户业务信息（IM），也可以往用户推送下行消息。多个后端业务系统可以接入到Route，按照服务类型（SVID、CMD）注册。

IM服务可以接入到Route, 注册SVID_IM或CMD_IM。这样Login接收到SVID=SVID_IM、或者CMD_IM的消息，转发给Route，Route就可以根据SVID转发给IM相关的服务。

Route只做转发，不处理具体的业务逻辑，因此也是无状态的。一个信令集群可以有多个Route服务，任何服务挂了不影响整体服务能力。

### 推送系统

接收服务端给用户发送下行消息的请求后，通过信令服务查询用户是否在线，如果在线走信令服务器进行在线推送，如果不在线走离线推送。

推送服务可能出现大规模并发蜂拥，可能会触发亿级的TPS。为此可以用Kafka或其他消息中间件做削峰。具体的服务实现模块可主要由以下4个部分组成:

- PushProxy：接受服务端给用户发送的下行消息，写入消息中间件
- 消息中间件(Kafka)：缓存推送服务消息并做削峰，并完成消息任务的推送
- PushServer：监听来自消息中间件的请求，消费消息，完成用的在线、离线状态的判断，调用PushWorker
- PushWorker：真正推送给信令或者APNS、Firebase、或其他推送平台

因此除了中间件以外的每个服务都是无状态的、因此也可以实现平行拓展和容错，任何服务挂点都不会对整体服务可用性产生影响。

### 持久化系统

当用户数据产生后，必然是需要伴随消息的存储和持久化的，因此持久化系统主要负责消息的存储和查询，已实现离线消息的拉取、漫游等的相关附属功能。因为消息量巨大，对存储服务的并发能力和存储量要求巨大。为了平衡性能、空间和成本，存储服务按数据的热度进行了分级和区别对待。

- 短期消息(7天)：存储在Redis里
- 近期消息(1-3个月)：存储在Mysql里，以备用户实时查询
- 历史信息：存储在HBase里，作为历史数据慢查询

为了应对超大群的大量消息处理，存储服务在实际的技术实现上，也做了比较细的分拆

**具体的业务划分**

- MsgProxy：负责接受业务系统的存储请求，写入消息中间件
-  MsgWriter：从消息中间件获取写请求，按需写入Redis和Mysql
- MsgReader：接受用户的消息查询请求，从Redis，Mysql或者HBase读数据

部署上可能是3-4个MsgProxy，后端可以对应15个左右的MsgWriter。MsgWriter是比较慢的，需要同时操作多个数据库，还要保证操作的原子性。